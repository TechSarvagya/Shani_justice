{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa045d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0.]\n",
      "epoch 10/50,loss=0.2522\n",
      "epoch 20/50,loss=0.0320\n",
      "epoch 30/50,loss=0.0096\n",
      "epoch 40/50,loss=0.0054\n",
      "epoch 50/50,loss=0.0081\n",
      "final loss=0.0081\n",
      "Let's chat! (type 'quit' to exit)\n",
      "You: can i be made to work if am under 14\n",
      "Child labor is strictly prohibited in India under Article 24 of the Constitution, which prohibits the employment of children below the age of fourteen years in any factory, mine, or hazardous occupation.\n",
      "You: why should i protect the environment\n",
      "Citizens play a vital role in safeguarding the environment by promoting conservation efforts and sustainable practices.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import json\n",
    "import random\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "def tokenize(statement):\n",
    "    return word_tokenize(statement)\n",
    "\n",
    "\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "def lemm(word):\n",
    "    return lemmatizer.lemmatize(word)\n",
    "ignore_words=[\",\",\".\",\"!\",\"?\"]\n",
    "\n",
    "def bag_of_words(sent,all_words):\n",
    "    bag=np.zeros(len(all_words))\n",
    "    for i,word in enumerate(all_words):\n",
    "        if word in sent:\n",
    "            bag[i]=1.0\n",
    "    return bag\n",
    "\n",
    "with open(\"Intents.json\",\"r\") as f:\n",
    "    intents=json.load(f)\n",
    "\n",
    "\n",
    "tags=[]\n",
    "all_words=[]\n",
    "xy=[]\n",
    "for intent in intents['intents']:\n",
    "     tag=intent['class']\n",
    "     tags.append(tag)\n",
    "     for pattern in intent['question_variations']: \n",
    "      sent=tokenize(pattern)\n",
    "      lemm_sent=[lemm(word) for word in sent if word not in ignore_words ]\n",
    "      all_words.extend(lemm_sent)\n",
    "      xy.append((lemm_sent,tag))\n",
    "all_words=sorted(set(all_words)) \n",
    "tags=sorted(set(tags)) \n",
    "X_train=[] \n",
    "y_train=[]\n",
    "\n",
    "for patterns,tag in xy:\n",
    "    y_train.append(tags.index(tag))\n",
    "    bag=bag_of_words(patterns,all_words)\n",
    "    X_train.append(bag)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "def lemm(word):\n",
    "    return lemmatizer.lemmatize(word)\n",
    "ignore_words=[\",\",\".\",\"!\",\"?\"]\n",
    "\n",
    "def bag_of_words(sent,all_words):\n",
    "    bag=np.zeros(len(all_words))\n",
    "    for i,word in enumerate(all_words):\n",
    "        if word in sent:\n",
    "            bag[i]=1.0\n",
    "    return bag\n",
    "\n",
    "with open(\"Intents.json\",\"r\") as f:\n",
    "    intents=json.load(f)\n",
    "\n",
    "\n",
    "tags=[]\n",
    "all_words=[]\n",
    "xy=[]\n",
    "for intent in intents['intents']:\n",
    "     tag=intent['class']\n",
    "     tags.append(tag)\n",
    "     for pattern in intent['question_variations']: \n",
    "      sent=tokenize(pattern)\n",
    "      lemm_sent=[lemm(word) for word in sent if word not in ignore_words ]\n",
    "      all_words.extend(lemm_sent)\n",
    "      xy.append((lemm_sent,tag))\n",
    "all_words=sorted(set(all_words)) \n",
    "tags=sorted(set(tags)) \n",
    "X_train=[] \n",
    "y_train=[]\n",
    "\n",
    "for patterns,tag in xy:\n",
    "    y_train.append(tags.index(tag))\n",
    "    bag=bag_of_words(patterns,all_words)\n",
    "    X_train.append(bag)\n",
    "\n",
    "X_train=np.array(X_train)\n",
    "y_train=np.array(y_train)\n",
    "print(X_train[0])\n",
    "\n",
    "class ChatDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.n_samples=len(X_train)\n",
    "        self.x_data=X_train\n",
    "        self.y_data=y_train\n",
    "    def __getitem__(self,idx):\n",
    "        return self.x_data[idx], self.y_data[idx]\n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self,input_size,hidden_size,num_classes):\n",
    "        super(NeuralNet,self).__init__()\n",
    "        self.l1=nn.Linear(input_size,hidden_size)\n",
    "        self.l2=nn.Linear(hidden_size,hidden_size)\n",
    "        self.l3=nn.Linear(hidden_size,hidden_size)\n",
    "        self.l4=nn.Linear(hidden_size,num_classes)\n",
    "        self.relu=nn.ReLU()\n",
    "    def forward(self,x):\n",
    "        out=self.l1(x)\n",
    "        out=self.relu(out)\n",
    "        out=self.l2(x)\n",
    "        out=self.relu(out)\n",
    "        out=self.l3(x)\n",
    "        out=self.relu(out)\n",
    "        out=self.l4(out)\n",
    "        return out\n",
    "         \n",
    "        \n",
    "\n",
    "data=ChatDataset()\n",
    "train_loader=DataLoader(dataset=data,batch_size=32,shuffle=True)\n",
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model=NeuralNet(len(X_train[0]),745,len(tags)).to(device)\n",
    "criterion=nn.CrossEntropyLoss()\n",
    "opt=torch.optim.Adam(model.parameters(),0.001)\n",
    "\n",
    "\n",
    "for epoch in range(50):\n",
    "    for (words,labels) in train_loader:\n",
    "        words=words.to(device)\n",
    "        labels=labels.to(device)\n",
    "        outputs=model(words.to(torch.float32))\n",
    "        loss=criterion(outputs,labels.type(torch.LongTensor))\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "    if (epoch+1)%10==0:\n",
    "        print(f\"epoch {epoch+1}/50,loss={loss.item():.4f}\")\n",
    "print(f\"final loss={loss.item():.4f}\")\n",
    "\n",
    "d ={'model_state':model.state_dict(),\n",
    " 'input_size':X_train[0],\n",
    " 'hidden_size':8,\n",
    " 'output_size':len(tags),\n",
    " 'all_words':all_words,\n",
    "  'tags':tags}\n",
    "torch.save(d,\"data.pth\")\n",
    "\n",
    "model.load_state_dict(d['model_state'])\n",
    "model.eval()\n",
    "def get_response(sentence):\n",
    "    sentence=tokenize(sentence)\n",
    "    X=bag_of_words(sentence,all_words)\n",
    "    X=X.reshape(1,X.shape[0])\n",
    "    X=torch.from_numpy(X).to(device)\n",
    "    output=model(X.to(torch.float32))\n",
    "    _,predicted=torch.max(output,dim=1)\n",
    "    tag=tags[predicted.item()]\n",
    "    probs=torch.softmax(output,dim=1)\n",
    "    prob=probs[0][predicted.item()]\n",
    "    if prob>=0.80:\n",
    "        for intent in intents['intents']:\n",
    "            if intent['class']==tag:\n",
    "                response=random.choice(intent['responses'])\n",
    "                return response\n",
    "    return \"I am sorry I could not understand you\"\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    print(\"Let's chat! (type 'quit' to exit)\")\n",
    "    while True:\n",
    "        sentence = input(\"You: \")\n",
    "        if sentence == \"quit\":\n",
    "            break\n",
    "\n",
    "        resp = get_response(sentence)\n",
    "        print(resp)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75040c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
